{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Data](#data)<br>\n",
    "    1.1. [Data Extraction](#data-extraction)<br>\n",
    "    1.2. [Manual Feature Selection](#manual-feature-selection)<br>\n",
    "    1.3. [Data Cleaning](#data-cleaning)<br>\n",
    "    1.4. [Generating New Features](#generating-new-features)<br>\n",
    "    &nbsp;&nbsp;&nbsp;1.4.1. [Sentiment Analysis](#sentiment-analysis)<br>\n",
    "    &nbsp;&nbsp;&nbsp;1.4.2. [Fix Duration](#fix-duration)<br>\n",
    "\n",
    "2. [Implementation](#implementation)<br>\n",
    "3. [Evaluation](#evaluation)<br>\n",
    "4. [Conclusion](#conclusion)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.cm as cm\n",
    "# import tensorflow as tf\n",
    "# import sklearn\n",
    "# import subprocess\n",
    "# import shlex\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data <a class=\"anchor\" id=\"data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Data Extraction <a class=\"anchor\" id=\"data-extraction\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_all_data = \"../dataset/data_test.csv\" # only using 10 examples\n",
    "\n",
    "df = pd.read_csv(path_all_data, sep=',')\n",
    "data = np.array(df)\n",
    "\n",
    "# TODO: Only use columns where the hand-selected features lie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Manual Feature Selection <a class=\"anchor\" id=\"manual-feature-selection\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the most relevant features are selected manually. The original features included in the dataset were the followig:\n",
    "\n",
    "<img src=\"./resources/features_in_dataset.png\" alt=\"Features in Dataset\" width=\"280\">\n",
    "\n",
    "However, after careful consideration, only the following columns will be taken into account for the purposes of this project:\n",
    "\n",
    "<font color=red>\n",
    "Need to perform manual feature selection!!!\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Data Cleaning <a class=\"anchor\" id=\"data-cleaning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> SIZE BEFORE CLEANING: (10, 32)\n",
      "> REMOVING ROW:\n",
      "[226 'Bug' 'Resolved' 'Fixed' '   ' 'Minor' 'Suresh Srinivas'\n",
      " '2012/05/11 18:29:32 +0100' 'null' 'Unassigned'\n",
      " '2012/05/15 03:31:24 +0100' 3.376296296 'null'\n",
      " 'Make the daemon names and other field names consistent   '\n",
      " 'Following names need to be consistent: Hdfs -&gt; HDFS Mapreduce -&gt; MapReduce Zookeeper -&gt; ZooKeeper HADOOP -&gt; Hadoop   '\n",
      " '0.9.0' '0.9.0' 0 1 20 0 1 1 1 0 0 0 0 0 0 0 nan]\n",
      "> SIZE AFTER CLEANING: (9, 32)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where data is missing\n",
    "print(\"> SIZE BEFORE CLEANING: \" + str(data.shape))\n",
    "for i, row in enumerate(data):\n",
    "    for j, val in enumerate(row):\n",
    "        if (row[j] == 'null'):\n",
    "            print(\"> REMOVING ROW:\\n\" + str(row))\n",
    "            data = np.delete(data, i, 0)\n",
    "            break\n",
    "            \n",
    "print(\"> SIZE AFTER CLEANING: \" + str(data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Generating New Features <a class=\"anchor\" id=\"generating-new-features\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1. Sentiment Analysis <a class=\"anchor\" id=\"sentiment-analysis\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://sentistrength.wlv.ac.uk/\n",
    "\n",
    "SentiStrength estimates the strength of positive and negative sentiment in short texts, even for informal language. It has human-level accuracy for short social web texts in English, except political texts. SentiStrength reports two sentiment strengths:\n",
    "\n",
    "-1 (not negative) to -5 (extremely negative)\n",
    "\n",
    "1 (not positive) to 5 (extremely positive)\n",
    "\n",
    "Why does it use two scores? Because research from psychology has revealed that we process positive and negative sentiment in parallel - hence mixed emotions.\n",
    "\n",
    "SentiStrength can also report binary (positive/negative), trinary (positive/negative/neutral) and single scale (-4 to +4) results. SentiStrength was originally developed for English and optimised for general short social web texts but can be configured for other languages and contexts by changing its input files - some variants are demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shlex\n",
    "import os.path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set the proper paths\n",
    "#SentiStrength\n",
    "#the SentiStrength data folder and to make this code work. These are near the top of the code below. The results will be saved to the folder where the YouTube files are kept. Only use forward slashes /.\n",
    "SentiStrengthLocation = \"../sentiment_analysis/SentiStrength/SentiStrength.jar\" #The location of SentiStrength on your computer\n",
    "SentiStrengthLanguageFolder = \"../sentiment_analysis/SentiStrength/data/\" #The location of the unzipped SentiStrength data files on your computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following code tests that the above three locations are correct. If you don't get an error message then this is fine.\n",
    "if not os.path.isfile(SentiStrengthLocation):\n",
    "    print(\"SentiStrength not found at: \", SentiStrengthLocation)\n",
    "if not os.path.isdir(SentiStrengthLanguageFolder):\n",
    "    print(\"SentiStrength data folder not found at: \", SentiStrengthLanguageFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The code below allows SentiStrength to be called and run on a single line of text.\n",
    "def RateSentiment(sentiString):\n",
    "    #open a subprocess using shlex to get the command line string into the correct args list format\n",
    "    p = subprocess.Popen(shlex.split(\"java -jar '\" + SentiStrengthLocation + \"' stdin sentidata '\" + SentiStrengthLanguageFolder + \"'\"),stdin=subprocess.PIPE,stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n",
    "    #communicate via stdin the string to be rated. Note that all spaces are replaced with +\n",
    "    b = bytes(sentiString.replace(\" \",\"+\"), 'utf-8') #Can't send string in Python 3, must send bytes\n",
    "    stdout_byte, stderr_text = p.communicate(b)\n",
    "    stdout_text = stdout_byte.decode(\"utf-8\")  #convert from byte\n",
    "    # -------- Edit - Nov 9 2017 --------\n",
    "    stdout_list = stdout_text.split(\"\\t\")      #Split by tab: ['2', '-1','\\n']\n",
    "    del stdout_list[-1]                        #Get rid of the last newline element: ['2', '-1']\n",
    "    results = list(map(int, stdout_list))      #Conver the characters to integers\n",
    "    results = results[0] + results[1]          #Combine the positive and the negative\n",
    "    # -------- END: Edit - Nov 9 2017 --------\n",
    "    #stdout_text = stdout_text.rstrip().replace(\"\\t\",\" \") #remove the tab spacing between the positive and negative ratings. e.g. 1    -5 -> 1 -5\n",
    "    #return stdout_text + \" \" + sentiString\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#Test to ensure that it works\n",
    "print(RateSentiment(\"i am happy but not really\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2. Fix Duration <a class=\"anchor\" id=\"fix-duration\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementation <a class=\"anchor\" id=\"implementation\"></a>\n",
    "\n",
    "*Note: Automatic Feature Selection will be included here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation <a class=\"anchor\" id=\"evaluation\"></a>\n",
    "\n",
    "*Note: Include comparisson with other related work*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Conclusion <a class=\"anchor\" id=\"conclusion\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
