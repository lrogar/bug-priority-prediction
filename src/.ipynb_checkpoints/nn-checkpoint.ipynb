{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data Extraction\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean_data_path = \"../dataset/clean_data.csv\"\n",
    "clean_data_path = \"../dataset/_ambari_clean_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def split_data(data, labels, train_perc):\n",
    "    \n",
    "    test_perc = round(1-train_perc, 2)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels, train_size=train_perc,\n",
    "                                                        test_size=test_perc, random_state=42, stratify=labels)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> data matrix shape: (1000, 5)\n",
      "> labels (y) shape: (1000, 5)\n",
      "> m (training samples) = 100\n",
      "> n (num. features)= 5\n",
      "> k (num. classes) = 5\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(clean_data_path, sep=',', encoding='ISO-8859-1', header=None)\n",
    "clean_data = np.array(df)\n",
    "\n",
    "# get rid of rows containing \"nan\" in clean data file\n",
    "rows_to_delete = []\n",
    "for i, row in enumerate(clean_data):\n",
    "    for j, val in enumerate(row):\n",
    "        if (str(row[j]).strip() == 'nan'):\n",
    "            print(\"> Deleting row: \" + str(row))\n",
    "            rows_to_delete.append(i)\n",
    "            break\n",
    "clean_data = np.delete(clean_data, rows_to_delete, 0)\n",
    "\n",
    "# don't include the last column; where the labels are\n",
    "data = (clean_data[:,:-1])\n",
    "\n",
    "# reshape from (m,) to (m,1), then convert into one-hot vector (m,k)\n",
    "y = pd.get_dummies(clean_data[:,-1]).values # also converting to one-hot vector using pandas\n",
    "\n",
    "print(\"> data matrix shape: \" + str(data.shape))\n",
    "print(\"> labels (y) shape: \" + str(y.shape))\n",
    "\n",
    "train_perc = .1 # percentage of total data used for training\n",
    "x_train, x_test, y_train, y_test = split_data(data, y, train_perc) # randomly splitting up the data\n",
    "m = x_train.shape[0] # number of tuples for training\n",
    "n = data.shape[1] # number of features\n",
    "k = len(y[0]) # number of classes\n",
    "\n",
    "print(\"> m (training samples) = \" + str(m) + \"\\n> n (num. features)= \" + str(n) + \"\\n> k (num. classes) = \" + str(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> y_rand shape: (900, 5)\n"
     ]
    }
   ],
   "source": [
    "y_rand = pd.get_dummies((np.floor(np.random.rand(len(y_test), 1)*5).astype(int)).flatten()).values\n",
    "print(\"> y_rand shape: \" + str(y_rand.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network (Luisa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_activation_function(X, W, b, func='softmax'):\n",
    "    \n",
    "    if (func == 'softmax'): # softmax\n",
    "       \n",
    "        return tf.nn.softmax(tf.add(tf.matmul(X, W), b))\n",
    "    \n",
    "    if (func == 'relu'): # relu\n",
    "        \n",
    "        return tf.nn.relu(tf.add(tf.matmul(X, W), b))\n",
    "\n",
    "    else: # sigmoid\n",
    "    \n",
    "        return tf.sigmoid(tf.add(tf.matmul(X, W), b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cost(y, y_):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using multiple layers\n",
    "def get_output_layer(n_hidden_layers, X, n, k, n_perceptrons):\n",
    "    \n",
    "    layer_weights = []\n",
    "    \n",
    "    # input layer to first hidden layer\n",
    "    layer_weights.append({'W': tf.Variable(tf.random_normal([n, n_perceptrons])),\n",
    "                          'b': tf.Variable(tf.random_normal([n_perceptrons]))})\n",
    "    \n",
    "    # generate this many hidden layers\n",
    "    for i in range(n_hidden_layers):\n",
    "        layer_weights.append({'W': tf.Variable(tf.random_normal([n_perceptrons, n_perceptrons])),\n",
    "                              'b': tf.Variable(tf.random_normal([n_perceptrons]))})\n",
    "\n",
    "    # last hidden layer to output layer\n",
    "    layer_weights.append({'W': tf.Variable(tf.random_normal([n_perceptrons, k])),\n",
    "                          'b': tf.Variable(tf.random_normal([k]))})\n",
    "            \n",
    "    # calculate output-first hidden inner layer\n",
    "    aggregated_val = apply_activation_function(X, layer_weights[0]['W'], layer_weights[0]['b'])\n",
    "    \n",
    "    # print(\"  aggregated_val.shape: \" + str(aggregated_val.shape))\n",
    "    \n",
    "    # calculate all hidden layers and output layer\n",
    "    for i in range(1, len(layer_weights)):\n",
    "        aggregated_val = apply_activation_function(aggregated_val, layer_weights[i]['W'], layer_weights[i]['b'])\n",
    "    \n",
    "    # return final layer\n",
    "    return aggregated_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(n_hidden_layers, X, y, n, learning_rate, epochs, k, init_perceptrons, total_perceptrons, step):\n",
    "   \n",
    "    # to store the different accuracy values for each number of perceptrons used\n",
    "    total_accuracy = []\n",
    "    \n",
    "    # if we are only trying with one set of perceptrons, adjust the upper bound for the \"range\" function below\n",
    "    if (init_perceptrons == total_perceptrons):\n",
    "        stop_cond = init_perceptrons + 1\n",
    "    # otherwise, set the upper bound taking into accout both the initial perceptrons, and the total wanted\n",
    "    else:\n",
    "        stop_cond = init_perceptrons + total_perceptrons + 1\n",
    "\n",
    "    # perform the training for each number of perceptrons specified\n",
    "    for n_nodes in range(init_perceptrons, stop_cond, step):\n",
    "\n",
    "        print(\"> Using \", n_nodes, \" perceptrons and \" + str(n_hidden_layers) + \" hidden layers ...\")\n",
    "\n",
    "        y_ = get_output_layer(n_hidden_layers, X, n, k, n_nodes)\n",
    "        cost_function = get_cost(y, y_)\n",
    "        \n",
    "        # using gradient descent to minimize the cost\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1)) # checking how many were predicted correctly\n",
    "        benchmark_prediction = tf.equal(tf.argmax(y_rand, 1), tf.argmax(y, 1))\n",
    "        \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        benchmark_accuracy = tf.reduce_mean(tf.cast(benchmark_prediction, tf.float32))\n",
    "\n",
    "        # --- TRAINING ---\n",
    "\n",
    "        # collecting cost for each epoch for plotting\n",
    "        total_cost = []\n",
    "        init_op = tf.global_variables_initializer()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            sess.run(init_op)\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "\n",
    "                _, c = sess.run([optimizer, cost_function], feed_dict={X:x_train, y:y_train})\n",
    "                total_cost.append(c)\n",
    "\n",
    "                if (epoch+1) % 1000 == 0:\n",
    "                    print(\"  EPOCH:\", (epoch+1), \"Cost =\", \"{:.15f}\".format(c))\n",
    "\n",
    "            a = sess.run(accuracy, feed_dict={X: x_test, y: y_test})\n",
    "            b_a = sess.run(benchmark_accuracy, feed_dict={y: y_test})\n",
    "            total_accuracy.append(a)\n",
    "            print(\"  >> Accuracy = \" + \"{:.5f}%\".format(a*100) + \" vs. Random = \" + \"{:.5f}%\".format(b_a*100))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden_layers = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 10000 # cycles of feed forward + backpropagation\n",
    "\n",
    "# used to observe the change in accuracy as number of perceptrons increases\n",
    "init_perceptrons = 200\n",
    "total_perceptrons = 200\n",
    "step = 25\n",
    "\n",
    "# declare training data placeholders\n",
    "X = tf.placeholder(tf.float32, [None, n]) # input x1, x2, x3, ..., x12 (12 nodes)\n",
    "y = tf.placeholder(tf.float32, [None, k]) # output (5 nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run model\n",
    "total_acc = run_model(n_hidden_layers, X, y, n, learning_rate, epochs, k, init_perceptrons,\n",
    "                        total_perceptrons, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network (Tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 0.01\n",
    "# epochs = 10000\n",
    "# batch_size = 100\n",
    "# num_perceptrons = 100\n",
    "\n",
    "# # declare training data placeholders\n",
    "# X = tf.placeholder(tf.float32, [None, n]) # input x1, x2, ..., x5 (5 nodes), features\n",
    "# y = tf.placeholder(tf.float32, [None, k]) # output (5 nodes), classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # now declare the weights connecting the input to the hidden layer\n",
    "# W1 = tf.Variable(tf.random_normal([n, num_perceptrons], stddev=0.03), name='W1')\n",
    "# b1 = tf.Variable(tf.random_normal([num_perceptrons]), name='b1')\n",
    "\n",
    "# # and the weights connecting the hidden layer to the output layer\n",
    "# W2 = tf.Variable(tf.random_normal([num_perceptrons, k], stddev=0.03), name='W2')\n",
    "# b2 = tf.Variable(tf.random_normal([k]), name='b2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate the output of the hidden layer\n",
    "# hidden_out = tf.add(tf.matmul(X, W1), b1)\n",
    "# hidden_out = tf.nn.relu(hidden_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # now calculate the hidden layer output - in this case, let's use a softmax activated\n",
    "# # output layer\n",
    "# y_ = tf.nn.softmax(tf.add(tf.matmul(hidden_out, W2), b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # limited between 1e-10 to 0.999999.  This is to make sure that we never get a case were we have a log(0) operation\n",
    "# y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\n",
    "# cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped) + (1 - y) * tf.log(1 - y_clipped), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # add an optimiser\n",
    "# optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # finally setup the initialisation operator\n",
    "# init_op = tf.global_variables_initializer()\n",
    "\n",
    "# # define an accuracy assessment operation\n",
    "# correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def next_batch(num, data, labels):\n",
    "    \n",
    "#     idx = np.arange(0 , len(data))\n",
    "#     np.random.shuffle(idx)\n",
    "#     idx = idx[:num]\n",
    "#     data_shuffle = [data[ i] for i in idx]\n",
    "#     labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "#     return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 cost = 0.684979320\n",
      "Epoch: 2000 cost = 0.606092155\n",
      "Epoch: 3000 cost = 0.522155821\n",
      "Epoch: 4000 cost = 0.474051654\n",
      "Epoch: 5000 cost = 0.446948349\n",
      "Epoch: 6000 cost = 0.406118393\n",
      "Epoch: 7000 cost = 0.390257120\n",
      "Epoch: 8000 cost = 0.408621430\n",
      "Epoch: 9000 cost = 0.379612952\n",
      "Epoch: 10000 cost = 0.337954223\n",
      "0.822222\n"
     ]
    }
   ],
   "source": [
    "# # start the session\n",
    "# with tf.Session() as sess:\n",
    "#     # initialise the variables\n",
    "#     sess.run(init_op)\n",
    "#     total_batch = int(len(x_train) / batch_size)\n",
    "#     for epoch in range(epochs):\n",
    "#         avg_cost = 0\n",
    "#         for i in range(total_batch):\n",
    "#             batch_x, batch_y = next_batch(batch_size, x_train, y_train)\n",
    "#             _, c = sess.run([optimiser, cross_entropy], feed_dict={X: batch_x, y: batch_y})\n",
    "#             avg_cost += c / total_batch\n",
    "        \n",
    "#         if (epoch+1) % 1000 == 0:\n",
    "#             print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.9f}\".format(avg_cost))\n",
    "        \n",
    "#     print(sess.run(accuracy, feed_dict={X: x_test, y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network (Alex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # neural network\n",
    "# num_epochs = 4000        # number of Epochs(forward+backward prop) to run\n",
    "# learning_rate = 0.001     # learning rate of the optimizers\n",
    "# HL_size = 5            # number of perceptrons in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def model(data, num_feat, num_class, HL_size):\n",
    "#     hidden_1_layer = {'weights':tf.Variable(tf.random_normal([num_feat, HL_size])),\n",
    "#                       'biases': tf.Variable(tf.random_normal([HL_size]))}\n",
    "    \n",
    "#     hidden_2_layer = {'weights':tf.Variable(tf.random_normal([HL_size, HL_size])),\n",
    "#                       'biases': tf.Variable(tf.random_normal([HL_size]))}\n",
    "    \n",
    "#     hidden_3_layer = {'weights':tf.Variable(tf.random_normal([HL_size, HL_size])),\n",
    "#                       'biases': tf.Variable(tf.random_normal([HL_size]))}\n",
    "    \n",
    "#     output_layer = {'weights':tf.Variable(tf.random_normal([HL_size, num_class])),\n",
    "#                       'biases': tf.Variable(tf.random_normal([num_class]))}\n",
    "    \n",
    "#     # (input_data * weights) + biases\n",
    "    \n",
    "#     l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "#     l1 = tf.nn.relu(l1)\n",
    "    \n",
    "#     l2 = tf.add(tf.matmul(l1, hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "#     l2 = tf.nn.relu(l2)\n",
    "    \n",
    "#     l3 = tf.add(tf.matmul(l2, hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "#     l3 = tf.nn.relu(l3)\n",
    "    \n",
    "#     output = tf.add(tf.matmul(l3, output_layer['weights']), output_layer['biases'])\n",
    "    \n",
    "#     return output\n",
    "\n",
    "# def run_neural_net(train_x, test_x, train_y, test_y):\n",
    "#     #Get the number of features and number of classes\n",
    "#     num_feat, num_class = len(train_x[0,:]), len(train_y[0,:])\n",
    "    \n",
    "#     # height x width\n",
    "#     x = tf.placeholder('float',[None, num_feat])\n",
    "#     y = tf.placeholder('float')\n",
    "    \n",
    "#     #Run the model\n",
    "#     prediction = model(x, num_feat, num_class, HL_size)\n",
    "#     cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = prediction,labels = y))\n",
    "    \n",
    "#     # learning_default = 0.001\n",
    "#     optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)    \n",
    "    \n",
    "#     #The benchmark prediction\n",
    "#     benchmark_prediction = tf.equal(tf.argmax(y_rand, 1), tf.argmax(y, 1))\n",
    "    \n",
    "#     with tf.Session() as s:\n",
    "#         s.run(tf.global_variables_initializer())\n",
    "        \n",
    "#         for epoch in range(num_epochs):\n",
    "#             #print(\"epoch_x: \",epoch_x.shape,\"epoch_y:\",epoch_x.shape);\n",
    "#             _, epoch_loss = s.run([optimizer, cost], feed_dict = {x:train_x, y:train_y})\n",
    "#             if (epoch+1) % 1000 == 0:\n",
    "#                 print('Epoch',epoch+1,'completed out of',num_epochs,'loss:',epoch_loss)\n",
    "            \n",
    "#         #Actual Prediction\n",
    "#         correct = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))        \n",
    "#         accuracy = tf.reduce_mean(tf.cast(correct,'float'))\n",
    "#         accuracy_val = accuracy.eval({x:test_x, y:test_y})\n",
    "        \n",
    "#         #Benchmark Prediction\n",
    "#         correct_bench = tf.equal(tf.argmax(prediction,1), tf.argmax(y_rand,1))\n",
    "#         accuracy_bench = tf.reduce_mean(tf.cast(correct_bench,'float'))\n",
    "#         accuracy_bench = accuracy_bench.eval({x:test_x, y:test_y})\n",
    "        \n",
    "#         print('Accuracy:', accuracy_val, \" Benchmark:\",accuracy_bench)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_neural_net(x_train, x_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
