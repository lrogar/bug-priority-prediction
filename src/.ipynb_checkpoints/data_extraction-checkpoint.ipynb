{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_data_cleaning = True\n",
    "testing = True # will only run the first 10 tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data Extraction\n",
    "import pandas as pd\n",
    "\n",
    "# SentiStrength\n",
    "import subprocess\n",
    "import shlex\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data <a class=\"anchor\" id=\"data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Data Extraction <a class=\"anchor\" id=\"data-extraction\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "\n",
    "    # read in data\n",
    "    df = pd.read_csv(path, sep=',', encoding='ISO-8859-1', header=None)\n",
    "    data = np.array(df)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Manual Feature Selection <a class=\"anchor\" id=\"manual-feature-selection\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manually_selected_features(data):\n",
    "    \n",
    "    print(\"> Getting manually selecting features...\")\n",
    "    \n",
    "    # cols to keep: 1, 5, 6, 13, 14, 19\n",
    "    # we're also keeping the priority column (5) for now\n",
    "    cols_to_delete = (0, 2, 3, 4, 7, 8, 9, 10, 11, 12, 15, 16, 17, 18, 20, 21,\n",
    "                        22, 23, 24, 25, 26, 27, 28, 29, 30, 31)\n",
    "    \n",
    "    data = np.delete(data, cols_to_delete, axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Data Cleaning <a class=\"anchor\" id=\"data-cleaning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "        \n",
    "    # ---------- Only keep columns selected manually ----------\n",
    "    \n",
    "    print('> Cleaning data...')\n",
    "    print(\"\\n  Tuples before data cleaning: \" + str(data[1:].shape[0]) + '\\n')\n",
    "    data = manually_selected_features(data)\n",
    "    \n",
    "    # ---------- Remove rows where data is missing ----------\n",
    "    \n",
    "    rows_to_delete = []\n",
    "\n",
    "    for i, row in enumerate(data):\n",
    "        for j, val in enumerate(row):\n",
    "            if (str(row[j]).strip() == 'null'):\n",
    "                # print(\"deleting row \" + str(i) + \": \" + str(row))\n",
    "                rows_to_delete.append(i)\n",
    "                break\n",
    "    \n",
    "    data = np.delete(data, rows_to_delete, 0)\n",
    "    # np.savetxt('../dataset/all_data_null_removed.csv', data, delimiter=',', fmt=\"%s\")\n",
    "\n",
    "    print(\"\\n  Tuples after data cleaning: \" + str(data[1:].shape[0]) + '\\n')\n",
    "        \n",
    "    # ---------- Split total data into design matrix and feature headers ----------\n",
    "        \n",
    "    # strip white space from features array and ignore headers in data matrix\n",
    "    feature_headers = [str(header).strip() for header in  data[0]] # remove white space around strings\n",
    "    data = data[1:] # excluding headers from data matrix\n",
    "    \n",
    "    # transform labels into integer encodings\n",
    "    labels = [str(val).strip() for val in  data[:,1]]\n",
    "    labels = LabelEncoder().fit_transform(labels)\n",
    "    \n",
    "    data = np.delete(data, 1, 1) # deleting labels column from data matrix\n",
    "    data = np.c_[data, labels] # add labels column to the end\n",
    "    \n",
    "    # remove \"priority\" header - these are the labels, and have already been extracted.\n",
    "    feature_headers=np.delete(feature_headers, 1)\n",
    "    \n",
    "    # Quantify issue \"type\" (0) and \"reporter\" (1)\n",
    "    data[:,0] = quantify_to_int(data[:,0])\n",
    "    data[:,1] = quantify_to_int(data[:,1])\n",
    "\n",
    "    # Apply sentiment analysis to \"summary\" (2) and \"description\" (3) features\n",
    "    data[:,2] = get_sentiment_feature(data[:,2])\n",
    "    data[:,3] = get_sentiment_feature(data[:,3])\n",
    "    \n",
    "    # Convert \"description_words\" (4) from strings to integers\n",
    "    data[:,4] = [int(words) for words in data[:,4]]\n",
    "\n",
    "    return data, feature_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Generating New Features <a class=\"anchor\" id=\"generating-new-features\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1. Sentiment Analysis <a class=\"anchor\" id=\"sentiment-analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# allows SentiStrength to be called and ran on a single line of text.\n",
    "def rate_sentiment(senti_string):\n",
    "    \n",
    "    if senti_string == '': return 0\n",
    "    \n",
    "    # Set the proper paths\n",
    "    sentistrength_location = \"./resources/SentiStrength/SentiStrength.jar\" # The location of SentiStrength on your computer\n",
    "    sentistrength_language_folder = \"./resources/SentiStrength/data/\" # The location of the unzipped SentiStrength data files on your computer\n",
    "    \n",
    "    # Tests the paths are correct.\n",
    "    # An error will be displayed if there is an issue.\n",
    "    if not os.path.isfile(sentistrength_location):\n",
    "        print(\"SentiStrength not found at: \", sentistrength_location)\n",
    "    if not os.path.isdir(sentistrength_language_folder):\n",
    "        print(\"SentiStrength data folder not found at: \", sentistrength_language_folder)\n",
    "       \n",
    "    # Open a subprocess using shlex to get the command line string into the correct args list format\n",
    "    p = subprocess.Popen(shlex.split(\"java -jar '\" + sentistrength_location + \"' stdin sentidata '\" + sentistrength_language_folder + \"'\"),stdin=subprocess.PIPE,stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n",
    "    # Communicate via stdin the string to be rated. Note that all spaces are replaced with \"+\"\n",
    "    b = bytes(senti_string.replace(\" \",\"+\"), 'utf-8') # Can't send string in Python 3, must send bytes\n",
    "    stdout_byte, stderr_text = p.communicate(b)\n",
    "    stdout_text = stdout_byte.decode(\"utf-8\")  # Convert from byte\n",
    "    # -------- Edit - Nov 9 2017 --------\n",
    "    stdout_list = stdout_text.split(\"\\t\")      # Split by tab: ['2', '-1','\\n']\n",
    "    del stdout_list[-1]                        # Get rid of the last newline element: ['2', '-1']\n",
    "    results = list(map(int, stdout_list))      # Convert the characters to integers\n",
    "    results = results[0] + results[1]          # Combine the positive and the negative\n",
    "    # -------- END: Edit - Nov 9 2017 --------\n",
    "    #stdout_text = stdout_text.rstrip().replace(\"\\t\",\" \") # Remove the tab spacing between the positive and negative ratings. e.g. 1    -5 -> 1 -5\n",
    "    #return stdout_text + \" \" + senti_string\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Test to ensure that it works correctly\n",
    "print(rate_sentiment(\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a column of the data, return another that will include a representation of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = 'â–ˆ'):\n",
    "    \n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = '\\r')\n",
    "    \n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment_feature(strings):\n",
    "    \n",
    "    print(\"> Applying sentiment analysis...\")\n",
    "    l = len(strings)\n",
    "    results = np.zeros(l)\n",
    "    \n",
    "    # Initial call to print 0% progress\n",
    "    printProgressBar(0, l, prefix = '  Progress:', suffix = 'Complete', length = 50)\n",
    "    \n",
    "    for i, element in enumerate(strings):\n",
    "        results[i] = rate_sentiment(element.strip())\n",
    "        printProgressBar(i + 1, l, prefix = '  Progress:', suffix = 'Complete', length = 50)       \n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3. Quantify Features <a class=\"anchor\" id=\"quantify-features\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def quantify_to_int(array):\n",
    "    \n",
    "    print(\"> Quantifying feature...\")\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    results = label_encoder.fit_transform(array)\n",
    "                \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.4. Adjust Given Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are provided in string format; however, we will need to convert them into one_hot vectors in order to use them as different classes in the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "def one_hot(array):\n",
    "    \n",
    "    print(\"> Transforming labels into one-hot vectors...\")\n",
    "    \n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    \n",
    "    # assuming array has already been transformed into integer encodings\n",
    "    # now, convert to binary (one-hot)\n",
    "    array = array.reshape(len(array), 1)\n",
    "    results = onehot_encoder.fit_transform(array)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementation <a class=\"anchor\" id=\"implementation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Fetch and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_path = \"../dataset/all_data.csv\"\n",
    "clean_data_path = \"../dataset/clean_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Cleaning data...\n",
      "\n",
      "  Tuples before data cleaning: 10\n",
      "\n",
      "> Getting manually selecting features...\n",
      "\n",
      "  Tuples after data cleaning: 10\n",
      "\n",
      "> Quantifying feature...\n",
      "> Quantifying feature...\n",
      "> Applying sentiment analysis...\n",
      "  Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0% Complete\n",
      "> Applying sentiment analysis...\n",
      "  Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0% Complete\n",
      "\n",
      "  Features considered: ['type' 'reporter' 'summary' 'description' 'description_words'] = 5\n",
      "\n",
      "  First 5 tuples example:\n",
      "[[1 3 0.0 0.0 1 4]\n",
      " [0 9 -2.0 0.0 245 2]\n",
      " [1 5 0.0 1.0 36 2]\n",
      " [0 2 0.0 0.0 17 1]\n",
      " [1 4 0.0 2.0 23 2]]\n"
     ]
    }
   ],
   "source": [
    "rows = 10\n",
    "if (not testing): rows=4000\n",
    "\n",
    "data, feature_headers = clean_data(get_data(all_data_path)[:rows + 1]) # +1 to include headers\n",
    "\n",
    "# Saving the clean data into a csv file for future use\n",
    "np.savetxt(clean_data_path, data, delimiter=',')\n",
    "\n",
    "tuples_to_print = 5\n",
    "print(\"\\n  Features considered: \" + str(feature_headers) + \" = \" + str(len(feature_headers)))\n",
    "print(\"\\n  First \" + str(tuples_to_print) + \" tuples example:\\n\" + str(data[:tuples_to_print]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Fetch Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def split_data(data, labels, train_perc):\n",
    "    \n",
    "    test_perc = round(1-train_perc, 2)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels, train_size=train_perc, test_size=test_perc, random_state=42)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Transforming labels into one-hot vectors...\n",
      "\n",
      "  data matrix shape: (10, 5)\n",
      "  labels (y) shape: (10, 5)\n",
      "\n",
      "> m (training samples) = 7\n",
      "> n (num. features)= 5\n",
      "> k (num. classes) = 5\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(clean_data_path, sep=',', encoding='ISO-8859-1', header=None)\n",
    "clean_data = np.array(df)\n",
    "\n",
    "# get rid of rows containing \"nan\" in clean data file\n",
    "rows_to_delete = []\n",
    "for i, row in enumerate(clean_data):\n",
    "    for j, val in enumerate(row):\n",
    "        if (str(row[j]).strip() == 'nan'):\n",
    "            print(\"> Deleting row: \" + str(row))\n",
    "            rows_to_delete.append(i)\n",
    "            break\n",
    "clean_data = np.delete(clean_data, rows_to_delete, 0)\n",
    "\n",
    "# don't include the last column; where the labels are\n",
    "data = (clean_data[:,:-1])\n",
    "\n",
    "# reshape from (m,) to (m,1), then convert into one-hot vector (m,k)\n",
    "y = one_hot((clean_data[:,-1]).reshape((-1, 1)))\n",
    "print(\"\\n  data matrix shape: \" + str(data.shape))\n",
    "print(\"  labels (y) shape: \" + str(y.shape) + '\\n')\n",
    "\n",
    "train_perc = .7 # percentage of total data used for training\n",
    "x_train, x_test, y_train, y_test = split_data(data, y, train_perc) # randomly splitting up the data\n",
    "m = x_train.shape[0] # number of tuples for training\n",
    "n = data.shape[1] # number of features\n",
    "k = len(y[0]) # number of classes\n",
    "\n",
    "print(\"> m (training samples) = \" + str(m) + \"\\n> n (num. features)= \" + str(n) + \"\\n> k (num. classes) = \" + str(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Transforming labels into one-hot vectors...\n",
      "\n",
      "  y_rand shape: (3, 2)\n"
     ]
    }
   ],
   "source": [
    "y_rand = one_hot(np.floor(np.random.rand(len(y_test),1)*5).astype(int))\n",
    "print(\"\\n  y_rand shape: \" + str(y_rand.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
