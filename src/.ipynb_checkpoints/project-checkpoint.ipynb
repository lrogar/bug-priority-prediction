{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "Given any high impact bug, identify its priority.\n",
    "\n",
    "Methods in research of Software Engineering focus on predicting, localizing, and triaging bugs, but do not consider their impact or weight on the users and on the developers.\n",
    "\n",
    "For this reason, we want to distinguish different kinds of bugs by placing in them in different priority categories: **Critical**, **Blocker**, **Major**, **Minor**, **Trivial**.\n",
    "\n",
    "Below are their definitions$^1$:\n",
    "\n",
    "* Blocker: we can not move forward until this task completed or this bug fixed. Blocker JIRAs block the completion of a design or development iteration, or the releaseof the product.\n",
    "    * development or design of a critical component or activity on the project can not move forward until this is resolved.\n",
    "    * breaks the build process.\n",
    "    * causes the UI to not start up or function at all.\n",
    "    * renders an area of finished functionality unusable for most users.\n",
    "    * not completed task which is needed for the kernel to function.\n",
    "* Critical: this task or issue will most likely be resolved for release.\n",
    "    * needed for the next iteration\n",
    "    * major browser incompatibility\n",
    "    * renders an isolated particular feature completely unusable\n",
    "    * affects kernel - front-end communication\n",
    "    * major accessibility issue\n",
    "    * major security issue\n",
    "* Major:(default) Issue should be resolved for release.\n",
    "    * majority of bugs and tasks\n",
    "    * if unsure, chose this\n",
    "* Minor:issue might be resolved for release.\n",
    "    * issues and bugs which can be fixed with entry level knowledge\n",
    "    * does not affect functionality in any way, presentation may be broken\n",
    "* Trivial: issue may or may not be resolved.\n",
    "    * this should be for borderline tasks and issues. For that moment when you're not sure whether you should write something up or not.\n",
    "\n",
    "<br><font color=red>\n",
    "*Note: Consider using regularization to penalize certain parameters in order to obtain more balanced data.*\n",
    "</font>\n",
    "\n",
    "Possible research questions:\n",
    "\n",
    "- Can we predict the priority of a bug given their summary, description, and type?\n",
    "- Can suggest an assignee to fix the bug given a bug and its priority?\n",
    "    - Can we predict the time it will take for the bug to be resolved?\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "`//TODO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_data_cleaning = True\n",
    "run_nn = False\n",
    "testing = True # will only run the first 10 tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data Extraction\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# SentiStrength\n",
    "import subprocess\n",
    "import shlex\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data <a class=\"anchor\" id=\"data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with a large dataset of high impact bugs, which was created by manually reviewing four thousand issue reports in four open source projects (Ambari, Camel, Derby and Wicket). The projects were extracted from JIRA, a platform for managing reported issues.\n",
    "\n",
    "There are 1000 examples per project; there will be 4000 examples to work with in total.\n",
    "\n",
    "These projects were selected because they met the following criteria for project selection:\n",
    "\n",
    "* Target projects have a large number of (at least several thousand) reported issues , which enables the use for prediction model building and/or machine learning.\n",
    "\n",
    "* Target projects use JIRA as an issue tracking system.\n",
    "\n",
    "* Target projects are different from each other in application domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Data Extraction <a class=\"anchor\" id=\"data-extraction\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "\n",
    "    # read in data\n",
    "    df = pd.read_csv(path, sep=',', encoding='ISO-8859-1')\n",
    "    data = np.array(df)    \n",
    "    \n",
    "    # get headers with feature labels\n",
    "    with open(path, 'r', newline='', encoding=\"ISO-8859-1\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        feature_headers = next(reader)\n",
    "        \n",
    "    # merge data and feature headers\n",
    "    data = np.insert(data, 0, feature_headers, 0)  \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Manual Feature Selection <a class=\"anchor\" id=\"manual-feature-selection\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the most relevant features are selected manually. The original features included in the dataset were the following:\n",
    "\n",
    "<img src=\"./resources/images/features-in-dataset.png\" width=\"300\">\n",
    "\n",
    "However, after careful consideration, only the following columns will be taken into account for further analysis:\n",
    "\n",
    "<img src=\"./resources/images/manual-feature-selection-table.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manually_selected_features(data):\n",
    "    \n",
    "    print(\"> Getting manually selecting features...\")\n",
    "    \n",
    "    # cols to keep: 1, 5, 6, 13, 14, 19\n",
    "    # we're also keeping the priority column (5) for now\n",
    "    cols_to_delete = (0, 2, 3, 4, 7, 8, 9, 10, 11, 12, 15, 16, 17, 18, 20, 21,\n",
    "                        22, 23, 24, 25, 26, 27, 28, 29, 30, 31)\n",
    "    \n",
    "    data = np.delete(data, cols_to_delete, axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Data Cleaning <a class=\"anchor\" id=\"data-cleaning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    \n",
    "    # ---------- Only keep columns selected manually ----------\n",
    "    \n",
    "    print('> Cleaning data...')\n",
    "    print(\"\\n  Tuples before data cleaning: \" + str(data[1:].shape[0]) + '\\n')\n",
    "    data = manually_selected_features(data)\n",
    "    \n",
    "    # ---------- Remove rows where data is missing ----------\n",
    "\n",
    "    rows_to_delete = []\n",
    "\n",
    "    for i, row in enumerate(data):\n",
    "        for j, val in enumerate(row):\n",
    "            if (str(row[j]).strip() == 'null'):\n",
    "                # print(\"deleting row \" + str(i) + \": \" + str(row))\n",
    "                rows_to_delete.append(i)\n",
    "                break\n",
    "    \n",
    "    data = np.delete(data, rows_to_delete, 0)\n",
    "    # np.savetxt('../dataset/all_data_null_removed.csv', data, delimiter=',', fmt=\"%s\")\n",
    "\n",
    "    print(\"\\n  Tuples after data cleaning: \" + str(data[1:].shape[0]) + '\\n')\n",
    "        \n",
    "    # ---------- Split total data into design matrix and feature headers ----------\n",
    "    \n",
    "    # extract labels (y) column from total dataset\n",
    "    # labels = one_hot(data[1:,1])\n",
    "    \n",
    "    # transform labels into integer encodings\n",
    "    labels = [str(val).strip() for val in  data[:,1]]\n",
    "    labels = LabelEncoder().fit_transform(labels)\n",
    "    \n",
    "    data = np.delete(data, 1, 1) # deleting labels column from data matrix\n",
    "    data = np.c_[data, labels] # add labels column to the end\n",
    "\n",
    "    # strip white space from features array and ignore headers in data matrix\n",
    "    feature_headers = [str(header).strip() for header in  data[0]] # remove white space around strings\n",
    "    data = data[1:] # excluding headers from data matrix\n",
    "\n",
    "    # Quantify issue \"type\" (0) and \"reporter\" (1)\n",
    "    data[:,0] = quantify_to_int(data[:,0])\n",
    "    data[:,1] = quantify_to_int(data[:,1])\n",
    "\n",
    "    # Apply sentiment analysis to \"summary\" (2) and \"description\" (3) features\n",
    "    data[:,2] = get_sentiment_feature(data[:,2])\n",
    "    data[:,3] = get_sentiment_feature(data[:,3])\n",
    "    \n",
    "    return data, feature_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Generating New Features <a class=\"anchor\" id=\"generating-new-features\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1. Sentiment Analysis <a class=\"anchor\" id=\"sentiment-analysis\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SentiStrength](http://sentistrength.wlv.ac.uk) estimates the strength of positive and negative sentiment in short texts, even for informal language. It has human-level accuracy for short social web texts in English, except political texts. SentiStrength reports two sentiment strengths:\n",
    "\n",
    "<br><center>$-1$ *(not negative)* to $-5$ *(extremely negative)*</center>\n",
    "\n",
    "<center>$1$ *(not positive)* to $5$ *(extremely positive)*</center><br>\n",
    "\n",
    "For this project, they will both be added in order to be used as a new column for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# allows SentiStrength to be called and ran on a single line of text.\n",
    "def rate_sentiment(senti_string):\n",
    "    \n",
    "    if senti_string == '': return 0\n",
    "    \n",
    "    # Set the proper paths\n",
    "    sentistrength_location = \"./resources/SentiStrength/SentiStrength.jar\" # The location of SentiStrength on your computer\n",
    "    sentistrength_language_folder = \"./resources/SentiStrength/data/\" # The location of the unzipped SentiStrength data files on your computer\n",
    "    \n",
    "    # Tests the paths are correct.\n",
    "    # An error will be displayed if there is an issue.\n",
    "    if not os.path.isfile(sentistrength_location):\n",
    "        print(\"SentiStrength not found at: \", sentistrength_location)\n",
    "    if not os.path.isdir(sentistrength_language_folder):\n",
    "        print(\"SentiStrength data folder not found at: \", sentistrength_language_folder)\n",
    "       \n",
    "    # Open a subprocess using shlex to get the command line string into the correct args list format\n",
    "    p = subprocess.Popen(shlex.split(\"java -jar '\" + sentistrength_location + \"' stdin sentidata '\" + sentistrength_language_folder + \"'\"),stdin=subprocess.PIPE,stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n",
    "    # Communicate via stdin the string to be rated. Note that all spaces are replaced with \"+\"\n",
    "    b = bytes(senti_string.replace(\" \",\"+\"), 'utf-8') # Can't send string in Python 3, must send bytes\n",
    "    stdout_byte, stderr_text = p.communicate(b)\n",
    "    stdout_text = stdout_byte.decode(\"utf-8\")  # Convert from byte\n",
    "    # -------- Edit - Nov 9 2017 --------\n",
    "    stdout_list = stdout_text.split(\"\\t\")      # Split by tab: ['2', '-1','\\n']\n",
    "    del stdout_list[-1]                        # Get rid of the last newline element: ['2', '-1']\n",
    "    results = list(map(int, stdout_list))      # Convert the characters to integers\n",
    "    results = results[0] + results[1]          # Combine the positive and the negative\n",
    "    # -------- END: Edit - Nov 9 2017 --------\n",
    "    #stdout_text = stdout_text.rstrip().replace(\"\\t\",\" \") # Remove the tab spacing between the positive and negative ratings. e.g. 1    -5 -> 1 -5\n",
    "    #return stdout_text + \" \" + senti_string\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Test to ensure that it works correctly\n",
    "print(rate_sentiment(\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a column of the data, return another that will include a representation of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "    \"\"\"\n",
    "    \n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = '\\r')\n",
    "    \n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment_feature(strings):\n",
    "    \n",
    "    print(\"> Applying sentiment analysis...\")\n",
    "    l = len(strings)\n",
    "    results = np.zeros(l)\n",
    "    \n",
    "    # Initial call to print 0% progress\n",
    "    printProgressBar(0, l, prefix = '  Progress:', suffix = 'Complete', length = 50)\n",
    "    \n",
    "    for i, element in enumerate(strings):\n",
    "        results[i] = rate_sentiment(element.strip())\n",
    "        printProgressBar(i + 1, l, prefix = '  Progress:', suffix = 'Complete', length = 50)       \n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3. Quantify Features <a class=\"anchor\" id=\"quantify-features\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def quantify_to_int(array):\n",
    "    \n",
    "    print(\"> Quantifying feature...\")\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    results = label_encoder.fit_transform(array)\n",
    "                \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.4. Adjust Given Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are provided in string format; however, we will need to convert them into one_hot vectors in order to use them as different classes in the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "def one_hot(array):\n",
    "    \n",
    "    print(\"> Transforming labels into one-hot vectors...\")\n",
    "    \n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    \n",
    "    # assuming array has already been transformed into integer encodings\n",
    "    # now, convert to binary (one-hot)\n",
    "    array = array.reshape(len(array), 1)\n",
    "    results = onehot_encoder.fit_transform(array)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementation <a class=\"anchor\" id=\"implementation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Fetch and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_path = \"../dataset/all_data.csv\"\n",
    "clean_data_path = \"../dataset/clean_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Cleaning data...\n",
      "\n",
      "  Tuples before data cleaning: 10\n",
      "\n",
      "> Getting manually selecting features...\n",
      "\n",
      "  Tuples after data cleaning: 10\n",
      "\n",
      "> Quantifying feature...\n",
      "> Quantifying feature...\n",
      "> Applying sentiment analysis...\n",
      "  Progress: |██████████████████████████████████████████████████| 100.0% Complete\n",
      "> Applying sentiment analysis...\n",
      "  Progress: |██████████████████████████████████████████████████| 100.0% Complete\n",
      "\n",
      "  Features considered: ['type', 'reporter', 'summary', 'description', 'description_words', '2'] = 6\n",
      "\n",
      "  First 5 tuples example:\n",
      "[[0 9 -2.0 0.0 245 0]\n",
      " [1 3 0.0 1.0 36 0]\n",
      " [1 2 0.0 2.0 23 0]\n",
      " [0 4 -1.0 1.0 187 0]\n",
      " [0 7 0.0 0.0 70 1]]\n"
     ]
    }
   ],
   "source": [
    "rows = 11\n",
    "if (not testing): rows=4001\n",
    "\n",
    "data, feature_headers = clean_data(get_data(all_data_path)[:11])\n",
    "\n",
    "# Saving the clean data into a csv file for future use\n",
    "np.savetxt(clean_data_path, data, delimiter=',')\n",
    "\n",
    "tuples_to_print = 5\n",
    "print(\"\\n  Features considered: \" + str(feature_headers) + \" = \" + str(len(feature_headers)))\n",
    "print(\"\\n  First \" + str(tuples_to_print) + \" tuples example:\\n\" + str(data[:tuples_to_print]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Fetch Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def split_data(data, labels, train_perc):\n",
    "    \n",
    "    test_perc = round(1-train_perc, 2)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels, train_size=train_perc, test_size=test_perc, random_state=42)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Transforming labels into one-hot vectors...\n",
      "  data shape: (3905, 12)\n",
      "  y shape: (3905, 5)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(clean_data_path, sep=',', encoding='ISO-8859-1')\n",
    "clean_data = np.array(df)\n",
    "\n",
    "# get rid of rows containing \"nan\" in clean data file\n",
    "rows_to_delete = []\n",
    "for i, row in enumerate(clean_data):\n",
    "    for j, val in enumerate(row):\n",
    "        if (str(row[j]).strip() == 'nan'):\n",
    "            rows_to_delete.append(i)\n",
    "            break\n",
    "clean_data = np.delete(clean_data, rows_to_delete, 0)\n",
    "\n",
    "# don't include the last column; where the labels are\n",
    "data = (clean_data[:,:-1])\n",
    "\n",
    "# reshape from (m,) to (m,1), then convert into one-hot vector (m,k)\n",
    "y = one_hot((clean_data[:,-1]).reshape((-1, 1)))\n",
    "print(\"  data shape: \" + str(data.shape))\n",
    "print(\"  y shape: \" + str(y.shape))\n",
    "\n",
    "train_perc = .7 # percentage of total data used for training\n",
    "x_train, x_test, y_train, y_test = split_data(data, y, train_perc) # randomly splitting up the data\n",
    "m = x_train.shape[0] # number of tuples for training\n",
    "n = data.shape[1] # number of features\n",
    "k = len(y[0]) # number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Transforming labels into one-hot vectors...\n",
      "  y_rand shape: (1172, 5)\n"
     ]
    }
   ],
   "source": [
    "y_rand = one_hot(np.floor(np.random.rand(len(y_test),1)*5).astype(int))\n",
    "print(\"  y_rand shape: \" + str(y_rand.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_activation_function(X, W, b, func='softmax'):\n",
    "    \n",
    "    if (func == 'softmax'): # softmax\n",
    "       \n",
    "        return tf.nn.softmax(tf.add(tf.matmul(X, W), b))\n",
    "    \n",
    "    if (func == 'relu'): # relu\n",
    "        \n",
    "        return tf.nn.relu(tf.add(tf.matmul(X, W), b))\n",
    "\n",
    "    else: # sigmoid\n",
    "    \n",
    "        return tf.sigmoid(tf.add(tf.matmul(X, W), b))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cost(y, y_, epsilon):\n",
    "    # return (-tf.reduce_mean(y * tf.log(y_ + epsilon) + (1 - y) * tf.log(1 - y_ + epsilon)))\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y))\n",
    "    # return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using multiple layers\n",
    "def get_output_layer(n_hidden_layers, X, n, k, n_perceptrons):\n",
    "    \n",
    "    layer_weights = []\n",
    "    \n",
    "    # input layer to first hidden layer\n",
    "    layer_weights.append({'W': tf.Variable(tf.random_normal([n, n_perceptrons])),\n",
    "                          'b': tf.Variable(tf.random_normal([n_perceptrons]))})\n",
    "    \n",
    "    # generate this many hidden layers\n",
    "    for i in range(n_hidden_layers):\n",
    "        layer_weights.append({'W': tf.Variable(tf.random_normal([n_perceptrons, n_perceptrons])),\n",
    "                              'b': tf.Variable(tf.random_normal([n_perceptrons]))})\n",
    "\n",
    "    # last hidden layer to output layer\n",
    "    layer_weights.append({'W': tf.Variable(tf.random_normal([n_perceptrons, k])),\n",
    "                          'b': tf.Variable(tf.random_normal([k]))})\n",
    "            \n",
    "    # calculate output-first hidden inner layer\n",
    "    aggregated_val = apply_activation_function(X, layer_weights[0]['W'], layer_weights[0]['b'])\n",
    "    \n",
    "    # print(\"  aggregated_val.shape: \" + str(aggregated_val.shape))\n",
    "    \n",
    "    # calculate all hidden layers and output layer\n",
    "    for i in range(1, len(layer_weights)):\n",
    "        aggregated_val = apply_activation_function(aggregated_val, layer_weights[i]['W'], layer_weights[i]['b'])\n",
    "    \n",
    "    # return final layer\n",
    "    return aggregated_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(n_hidden_layers, X, y, n, epsilon, learning_rate, epochs, k, init_perceptrons, total_perceptrons, step):\n",
    "   \n",
    "    # to store the different accuracy values for each number of perceptrons used\n",
    "    total_accuracy = []\n",
    "    \n",
    "    # if we are only trying with one set of perceptrons, adjust the upper bound for the \"range\" function below\n",
    "    if (init_perceptrons == total_perceptrons):\n",
    "        stop_cond = init_perceptrons + 1\n",
    "    # otherwise, set the upper bound taking into accout both the initial perceptrons, and the total wanted\n",
    "    else:\n",
    "        stop_cond = init_perceptrons + total_perceptrons + 1\n",
    "\n",
    "    # perform the training for each number of perceptrons specified\n",
    "    for n_nodes in range(init_perceptrons, stop_cond, step):\n",
    "\n",
    "        print(\"> Using \", n_nodes, \" perceptrons and \" + str(n_hidden_layers) + \" hidden layers ...\")\n",
    "\n",
    "        y_ = get_output_layer(n_hidden_layers, X, n, k, n_nodes)\n",
    "        cost_function = get_cost(y, y_, epsilon)\n",
    "        \n",
    "        # using gradient descent to minimize the cost\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1)) # checking how many were predicted correctly\n",
    "        benchmark_prediction = tf.equal(tf.argmax(y_rand, 1), tf.argmax(y, 1))\n",
    "        \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        benchmark_accuracy = tf.reduce_mean(tf.cast(benchmark_prediction, tf.float32))\n",
    "\n",
    "        # --- TRAINING ---\n",
    "\n",
    "        # collecting cost for each epoch for plotting\n",
    "        total_cost = []\n",
    "        init_op = tf.global_variables_initializer()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            sess.run(init_op)\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "\n",
    "                _, c = sess.run([optimizer, cost_function], feed_dict={X:x_train, y:y_train})\n",
    "                total_cost.append(c)\n",
    "\n",
    "                if (epoch+1) % 1000 == 0:\n",
    "                    print(\"  EPOCH:\", (epoch+1), \"Cost =\", \"{:.15f}\".format(c))\n",
    "\n",
    "            a = sess.run(accuracy, feed_dict={X: x_test, y: y_test})\n",
    "            b_a = sess.run(benchmark_accuracy, feed_dict={y: y_test})\n",
    "            total_accuracy.append(a)\n",
    "            print(\"  >> Accuracy = \" + \"{:.5f}%\".format(a*100) + \" vs. Random = \" + \"{:.5f}%\".format(b_a*100))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set variables and placeholders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden_layers = 5\n",
    "learning_rate = 0.01\n",
    "epochs = 10000 # cycles of feed forward + backpropagation\n",
    "epsilon = 0.000001 # used to avoid \"nan\" values from log in cost function\n",
    "\n",
    "# used to observe the change in accuracy as number of perceptrons increases\n",
    "init_perceptrons = 250\n",
    "total_perceptrons = 250\n",
    "step = 25\n",
    "\n",
    "# declare training data placeholders\n",
    "X = tf.placeholder(tf.float32, [None, n]) # input x1, x2, x3, ..., x12 (12 nodes)\n",
    "y = tf.placeholder(tf.float32, [None, k]) # output (5 nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Using  250  perceptrons and 5 hidden layers ...\n",
      "  EPOCH: 1000 Cost = 1.616793155670166\n",
      "  EPOCH: 2000 Cost = 1.525791883468628\n",
      "  EPOCH: 3000 Cost = 1.377161026000977\n",
      "  EPOCH: 4000 Cost = 1.310011029243469\n",
      "  EPOCH: 5000 Cost = 1.287080168724060\n",
      "  EPOCH: 6000 Cost = 1.276657462120056\n",
      "  EPOCH: 7000 Cost = 1.270870923995972\n",
      "  EPOCH: 8000 Cost = 1.267209649085999\n",
      "  EPOCH: 9000 Cost = 1.264721989631653\n",
      "  EPOCH: 10000 Cost = 1.262904405593872\n",
      "  >> Accuracy = 65.10239% vs. Random = 21.50171%\n"
     ]
    }
   ],
   "source": [
    "if run_nn:\n",
    "    # run model\n",
    "    total_acc = run_model(n_hidden_layers, X, y, n, epsilon, learning_rate, epochs, k, init_perceptrons,\n",
    "                          total_perceptrons, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation <a class=\"anchor\" id=\"evaluation\"></a>\n",
    "\n",
    "*Note: Include comparisson with other related work*\n",
    "\n",
    "| Paper | Method |\n",
    "|-------|--------|\n",
    "| [Automated Identification of High Impact Bug<br>Reports Leveraging Imbalanced Learning Strategies](http://ieeexplore.ieee.org.uproxy.library.dc-uoit.ca/stamp/stamp.jsp?arnumber=7552013&tag=1 \"Paper\") |  Naive Bayes Multinominal |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Conclusion <a class=\"anchor\" id=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: https://confluence.ets.berkeley.edu/confluence/display/MYB/JIRA%2C+Definitions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
