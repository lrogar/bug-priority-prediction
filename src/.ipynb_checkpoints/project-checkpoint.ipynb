{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "Given any high impact bug, identify its priority.\n",
    "\n",
    "Methods in research of Software Engineering focus on predicting, localizing, and triaging bugs, but do not consider their impact or weight on the users and on the developers.\n",
    "\n",
    "For this reason, we want to distinguish different kinds of bugs by placing in them in different priority categories: **Critical**, **Blocker**, **Major**, **Minor**, **Trivial**.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Data](#data)<br>\n",
    "    1.1. [Data Extraction](#data-extraction)<br>\n",
    "    1.2. [Manual Feature Selection](#manual-feature-selection)<br>\n",
    "    1.3. [Data Cleaning](#data-cleaning)<br>\n",
    "    1.4. [Generating New Features](#generating-new-features)<br>\n",
    "    &nbsp;&nbsp;&nbsp;1.4.1. [Sentiment Analysis](#sentiment-analysis)<br>\n",
    "    &nbsp;&nbsp;&nbsp;1.4.2. [Number of Versions Affected](#number-versions)<br>\n",
    "    &nbsp;&nbsp;&nbsp;1.4.3. [Quantify Features](#quantify-features)<br>\n",
    "\n",
    "2. [Implementation](#implementation)<br>\n",
    "3. [Evaluation](#evaluation)<br>\n",
    "4. [Conclusion](#conclusion)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data Extraction\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# SentiStrength\n",
    "import subprocess\n",
    "import shlex\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data <a class=\"anchor\" id=\"data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with a large dataset of high impact bugs, which was created by manually reviewing four thousand issue reports in four open source projects (Ambari, Camel, Derby and Wicket). The projects were extracted from JIRA, a platform for managing reported issues.\n",
    "\n",
    "There are 1000 examples per project; there will be 4000 examples to work with in total.\n",
    "\n",
    "These projects were selected because they met the following criteria for project selection:\n",
    "\n",
    "* Target projects have a large number of (at least several thousand) reported issues , which enables the use for prediction model building and/or machine learning.\n",
    "\n",
    "* Target projects use JIRA as an issue tracking system.\n",
    "\n",
    "* Target projects are different from each other in application domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Data Extraction <a class=\"anchor\" id=\"data-extraction\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "\n",
    "    # read in data\n",
    "    df = pd.read_csv(path, sep=',', encoding='ISO-8859-1')\n",
    "    data = np.array(df)    \n",
    "    \n",
    "    # get headers with feature labels\n",
    "    with open(path, 'r', newline='', encoding=\"ISO-8859-1\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        feature_headers = next(reader)\n",
    "        \n",
    "    # merge data and feature headers\n",
    "    data = np.insert(data, 0, feature_headers, 0)  \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Manual Feature Selection <a class=\"anchor\" id=\"manual-feature-selection\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the most relevant features are selected manually. The original features included in the dataset were the following:\n",
    "\n",
    "<img src=\"./resources/images/features_in_dataset.png\" alt=\"Features in Dataset\" width=\"360\">\n",
    "\n",
    "However, after careful consideration, only the following columns will be taken into account for further analysis:\n",
    "\n",
    "| NAME | DESCRIPTION |\n",
    "|:-----|:------------|\n",
    "| `status` | Status of an issue (Resolved or Closed) |\n",
    "| `assignee` | Assignee's Name |\n",
    "| `time_fixed` | Time to fix an issue (assigned to resolved) |\n",
    "| `summary` | Summary of an issue |\n",
    "| `description` | Descriptions of an issue |\n",
    "| `affected version` | Versions affected by an issue |\n",
    "| `fixed_version` | Versions of a fixed issue |\n",
    "| `votes` | Number of votes |\n",
    "| `watches` | Number of watchers |\n",
    "| `description_words` | Number of words used in description |\n",
    "| `assignee_count` | Number of assignees |\n",
    "| `comment_count` | Number of comments for an issue |\n",
    "| `commenter_count` | Number of developers who comment on an issue |\n",
    "| `commit_count` | Number of commits to resolve an issue |\n",
    "\n",
    "Criteria for manual feature filtering:\n",
    "- Actually provided in the dataset\n",
    "- Possible influence on bug priority prediction\n",
    "- Lack of data\n",
    "- Non-quantifiable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manually_selected_features(data):\n",
    "    \n",
    "    print(\"> Getting manually selecting features...\")\n",
    "    \n",
    "    # note: we're keeping the priority column for now\n",
    "    data = np.delete(data,(0,1,3,4,6,7,8,10,11,15,16,23,24,25,26,27,28,30,31), axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Data Cleaning <a class=\"anchor\" id=\"data-cleaning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    \n",
    "    # ---------- Only keep columns selected manually ----------\n",
    "    \n",
    "    print('> Cleaning data...')\n",
    "    data = manually_selected_features(data)\n",
    "    \n",
    "    # ---------- Remove rows where data is missing ----------\n",
    "\n",
    "    rows_to_delete = []\n",
    "\n",
    "    for i, row in enumerate(data):\n",
    "        for j, val in enumerate(row):\n",
    "            if (str(row[j]).strip() == 'null'):\n",
    "                # print(\"deleting row \" + str(i) + \": \" + str(row))\n",
    "                rows_to_delete.append(i)\n",
    "                break\n",
    "    \n",
    "    data = np.delete(data, rows_to_delete, 0)\n",
    "    # np.savetxt('../dataset/all_data_null_removed.csv', data, delimiter=',', fmt=\"%s\")\n",
    "\n",
    "    print(\"\\n  Number of tuples after data cleaning: \" + str(data[1:].shape[0]) + '\\n')\n",
    "        \n",
    "    # ---------- Split total data into design matrix and feature headers ----------\n",
    "    \n",
    "    # extract labels (y) column from total dataset\n",
    "    # labels = one_hot(data[1:,1])\n",
    "    \n",
    "    # transform labels into integer encodings\n",
    "    labels = [str(val).strip() for val in  data[:,1]]\n",
    "    labels = LabelEncoder().fit_transform(labels)\n",
    "    \n",
    "    data = np.delete(data, 1, 1) # deleting labels column from data matrix\n",
    "    data = np.c_[data, labels] # add labels column to the end\n",
    "\n",
    "    # strip white space from features array and ignore headers in data matrix\n",
    "    feature_headers = [str(header).strip() for header in  data[0]] # remove white space around strings\n",
    "    data = data[1:] # excluding headers from data matrix\n",
    "\n",
    "    # rename some features for more descriptive names\n",
    "    feature_headers = [ft.replace('assigned', 'time_fix') for ft in feature_headers]\n",
    "    feature_headers = [ft.replace('commenter', 'commenter_count') for ft in feature_headers]\n",
    "\n",
    "    # Quantify \"status\" (0) and \"assignee\" (1)\n",
    "    data[:,0] = quantify_status(data[:,0])\n",
    "    data[:,1] = quantify_assignee(data[:,1])\n",
    "\n",
    "    # Converting time_fix from string to float\n",
    "    data[:,2] = data[:,2].astype(np.float)\n",
    "\n",
    "    # Apply sentiment analysis to \"summary\" (3) and \"description\" (4) features\n",
    "    data[:,3] = get_sentiment_feature(data[:,3])\n",
    "    data[:,4] = get_sentiment_feature(data[:,4])\n",
    "    \n",
    "    return data, feature_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Generating New Features <a class=\"anchor\" id=\"generating-new-features\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1. Sentiment Analysis <a class=\"anchor\" id=\"sentiment-analysis\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SentiStrength](http://sentistrength.wlv.ac.uk) estimates the strength of positive and negative sentiment in short texts, even for informal language. It has human-level accuracy for short social web texts in English, except political texts. SentiStrength reports two sentiment strengths:\n",
    "\n",
    "<br><center>$-1$ *(not negative)* to $-5$ *(extremely negative)*</center>\n",
    "\n",
    "<center>$1$ *(not positive)* to $5$ *(extremely positive)*</center><br>\n",
    "\n",
    "For this project, they will both be added in order to be used as a new column for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# allows SentiStrength to be called and ran on a single line of text.\n",
    "def rate_sentiment(senti_string):\n",
    "    \n",
    "    if senti_string == '': return 0\n",
    "    \n",
    "    # Set the proper paths\n",
    "    sentistrength_location = \"./resources/SentiStrength/SentiStrength.jar\" # The location of SentiStrength on your computer\n",
    "    sentistrength_language_folder = \"./resources/SentiStrength/data/\" # The location of the unzipped SentiStrength data files on your computer\n",
    "    \n",
    "    # Tests the paths are correct.\n",
    "    # An error will be displayed if there is an issue.\n",
    "    if not os.path.isfile(sentistrength_location):\n",
    "        print(\"SentiStrength not found at: \", sentistrength_location)\n",
    "    if not os.path.isdir(sentistrength_language_folder):\n",
    "        print(\"SentiStrength data folder not found at: \", sentistrength_language_folder)\n",
    "       \n",
    "    # Open a subprocess using shlex to get the command line string into the correct args list format\n",
    "    p = subprocess.Popen(shlex.split(\"java -jar '\" + sentistrength_location + \"' stdin sentidata '\" + sentistrength_language_folder + \"'\"),stdin=subprocess.PIPE,stdout=subprocess.PIPE,stderr=subprocess.PIPE)\n",
    "    # Communicate via stdin the string to be rated. Note that all spaces are replaced with \"+\"\n",
    "    b = bytes(senti_string.replace(\" \",\"+\"), 'utf-8') # Can't send string in Python 3, must send bytes\n",
    "    stdout_byte, stderr_text = p.communicate(b)\n",
    "    stdout_text = stdout_byte.decode(\"utf-8\")  # Convert from byte\n",
    "    # -------- Edit - Nov 9 2017 --------\n",
    "    stdout_list = stdout_text.split(\"\\t\")      # Split by tab: ['2', '-1','\\n']\n",
    "    del stdout_list[-1]                        # Get rid of the last newline element: ['2', '-1']\n",
    "    results = list(map(int, stdout_list))      # Convert the characters to integers\n",
    "    results = results[0] + results[1]          # Combine the positive and the negative\n",
    "    # -------- END: Edit - Nov 9 2017 --------\n",
    "    #stdout_text = stdout_text.rstrip().replace(\"\\t\",\" \") # Remove the tab spacing between the positive and negative ratings. e.g. 1    -5 -> 1 -5\n",
    "    #return stdout_text + \" \" + senti_string\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Test to ensure that it works correctly\n",
    "print(rate_sentiment(\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a column of the data, return another that will include a representation of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "    \"\"\"\n",
    "    \n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = '\\r')\n",
    "    \n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment_feature(strings):\n",
    "    \n",
    "    print(\"> Applying sentiment analysis...\")\n",
    "    l = len(strings)\n",
    "    results = np.zeros(l)\n",
    "    \n",
    "    # Initial call to print 0% progress\n",
    "    printProgressBar(0, l, prefix = '  Progress:', suffix = 'Complete', length = 50)\n",
    "    \n",
    "    for i, element in enumerate(strings):\n",
    "        results[i] = rate_sentiment(element.strip())\n",
    "        printProgressBar(i + 1, l, prefix = '  Progress:', suffix = 'Complete', length = 50)       \n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2. Number of Versions Affected <a class=\"anchor\" id=\"number-versions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_num_versions(v1, v2):\n",
    "    \n",
    "#     print(\"> Getting number of versions...\")\n",
    "    \n",
    "#     v1, v2 = v1.astype(np.float), v2.astype(np.float)\n",
    "#     return (np.subtract(v2, v1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3. Quantify Features <a class=\"anchor\" id=\"quantify-features\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def quantify_status(array):\n",
    "    \n",
    "    print(\"> Quantifying \\\"status\\\" feature...\")\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    results = label_encoder.fit_transform(array)\n",
    "                \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def quantify_assignee(array):\n",
    "    \n",
    "    print(\"> Quantifying \\\"assignee\\\" feature...\")\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    results = label_encoder.fit_transform(array)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.4. Adjust Given Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are provided in string format; however, we will need to convert them into one_hot vectors in order to use them as different classes in the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "def one_hot(array):\n",
    "    \n",
    "    print(\"> Transforming labels into one-hot vectors...\")\n",
    "    \n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    \n",
    "    # assuming array has already been transformed into integer encodings\n",
    "    # now, convert to binary (one-hot)\n",
    "    array = array.reshape(len(array), 1)\n",
    "    results = onehot_encoder.fit_transform(array)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementation <a class=\"anchor\" id=\"implementation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Fetch and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_path = \"../dataset/all_data.csv\"\n",
    "clean_data_path = \"../dataset/clean_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Cleaning data...\n",
      "> Getting manually selecting features...\n",
      "\n",
      "  Number of tuples after data cleaning: 940\n",
      "\n",
      "> Quantifying \"status\" feature...\n",
      "> Quantifying \"assignee\" feature...\n",
      "> Applying sentiment analysis...\n",
      "  Progress: |██████████████████████████████████████████████████| 100.0% Complete\n",
      "> Applying sentiment analysis...\n",
      "  Progress: |██████████████████████████████████████████████████| 100.0% Complete\n",
      "\n",
      "  Features considered: ['status', 'assignee', 'time_fix', 'summary', 'description', 'votes', 'watches', 'description_words', 'assingnee_count', 'comment_count', 'commenter_count', 'commit_count', '5'] = 13\n",
      "\n",
      "  Tuple example:\n",
      "[[0 0 353.0791898 ..., 2 0.0 2]\n",
      " [2 39 0.0 ..., 14 1.0 2]\n",
      " [2 18 0.001157407 ..., 1 0.0 2]\n",
      " ..., \n",
      " [0 11 0.008148148 ..., 2 1.0 3]\n",
      " [2 71 0.011030093 ..., 2 0.0 2]\n",
      " [0 57 0.014259259 ..., 6 1.0 2]]\n"
     ]
    }
   ],
   "source": [
    "data, feature_headers = clean_data(get_data(all_data_path))\n",
    "\n",
    "# Saving the clean data into a file for future use\n",
    "np.savetxt(clean_data_path, data, delimiter=',')\n",
    "\n",
    "print(\"\\n  Features considered: \" + str(feature_headers) + \" = \" + str(len(feature_headers)))\n",
    "print(\"\\n  Tuple example:\\n\" + str(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Fetch Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 13)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(clean_data_path, sep=',', encoding='ISO-8859-1')\n",
    "clean_data = np.array(df)\n",
    "\n",
    "test_data = (clean_data[:5,:])\n",
    "print(test_data.shape)\n",
    "\n",
    "# train_perc = .7 # percentage of total data used for training\n",
    "# x_train, x_test, y_train, y_test = split_data(data, labels, train_perc) # randomly splitting up the data\n",
    "# m = x_train.shape[0] # number of tuples for training\n",
    "# n = data.shape[1] # number of features\n",
    "# k = 5 # number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_activation_function(X, W, b, func='sigmoid'):\n",
    "    \n",
    "    if (func == 'softmax'): # softmax\n",
    "       \n",
    "        return tf.nn.softmax(tf.add(tf.matmul(X, W), b))\n",
    "\n",
    "    else: # sigmoid\n",
    "    \n",
    "        return tf.sigmoid(tf.add(tf.matmul(X, W), b))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cost(y, y_, epsilon):\n",
    "    return (-tf.reduce_mean(y * tf.log(y_ + epsilon) + (1 - y) * tf.log(1 - y_ + epsilon)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using one hidden layer\n",
    "def get_output_layer(X, n, k, n_perceptrons):\n",
    "    \n",
    "    # declare the weights connecting the input to the hidden layer\n",
    "    hidden_layer = {'W': tf.Variable(tf.random_normal([n, n_perceptrons])),\n",
    "                    'b': tf.Variable(tf.random_normal([n_perceptrons]))}\n",
    "\n",
    "    # declare the weights connecting the hidden layer to the output layer\n",
    "    output_layer = {'W': tf.Variable(tf.random_normal([n_perceptrons, k])),\n",
    "                    'b': tf.Variable(tf.random_normal([k]))}\n",
    "    \n",
    "    # calculate hidden layer\n",
    "    hidden_out = apply_activation_function(X, hidden_layer['W'], hidden_layer['b'])\n",
    "\n",
    "    # calculate output layer\n",
    "    y_ = apply_activation_function(hidden_out, output_layer['W'], output_layer['b'])\n",
    "    \n",
    "    return y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using multiple layers\n",
    "def get_output_layer2(X, n, k, n_perceptrons, n_hidden_layers=1):\n",
    "        \n",
    "    layer_weights = {}\n",
    "        \n",
    "    for i in range(n_hidden_layers): # generate this many hidden layers\n",
    "        \n",
    "        if (i == 0): # from input layer to first hidden layer\n",
    "        \n",
    "            layer_weights[i] = {'W': tf.Variable(tf.random_normal([n, n_perceptrons])),\n",
    "                                'b': tf.Variable(tf.random_normal([n_perceptrons]))}\n",
    "            \n",
    "        elif (i == (n_hidden_layers-1)): # from last hidden layer to output\n",
    "            \n",
    "            layer_weights[i] = {'W': tf.Variable(tf.random_normal([n_perceptrons, k])),\n",
    "                                'b': tf.Variable(tf.random_normal([k]))}\n",
    "\n",
    "        else: # just some hidden layer\n",
    "            \n",
    "            layer_weights[i] = {'W': tf.Variable(tf.random_normal([n_perceptrons, n_perceptrons])),\n",
    "                                'b': tf.Variable(tf.random_normal([n_perceptrons]))}\n",
    "            \n",
    "    # calculate output-first hidden inner layer\n",
    "    aggregated_val = apply_activation_function(X, layer_weights[0], layer_weights[0])\n",
    "    \n",
    "    # calculate all hidden layers and output layer\n",
    "    for i in range(1, len(layer_weights)):\n",
    "        aggregated_val = apply_activation_function(aggregated_val, layer_weights[i]['W'], layer_weights[i]['b'])\n",
    "    \n",
    "    # return final layer\n",
    "    return aggregated_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def split_data(data, labels, train_perc):\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels, train_size=train_perc, random_state=42)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>`run_model()`: **IN PROGRESS**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(X, y, n, epsilon, learning_rate, epochs, k=5, init_perceptrons=2, total_perceptrons=2, step=10):\n",
    "   \n",
    "    # to store the different accuracy values for each number of perceptrons used\n",
    "    total_accuracy = []\n",
    "    \n",
    "    # if we are only trying with one set of perceptrons, adjust the upper bound for the \"range\" function below\n",
    "    if (init_perceptrons == total_perceptrons):\n",
    "        stop_cond = init_perceptrons + 1\n",
    "    # otherwise, set the upper bound taking into accout both the initial perceptrons, and the total wanted\n",
    "    else:\n",
    "        stop_cond = init_perceptrons + total_perceptrons + 1\n",
    "\n",
    "    # perform the training for each number of perceptrons specified\n",
    "    for n_nodes in range(init_perceptrons, stop_cond, step):\n",
    "\n",
    "        print(\"> Using \", n_nodes, \" perceptrons in the hidden layer ...\")\n",
    "\n",
    "        y_ = get_output_layer(X, n, k, n_nodes)\n",
    "        cost_function = get_cost(y, y_, epsilon)\n",
    "        \n",
    "        # using gradient descent to minimize the cost\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n",
    "\n",
    "        # predicted_class = tf.greater(y_, 0.5) # used for BINARY classification only\n",
    "        correct_prediction = tf.equal(predicted_class, tf.cast(y, 'bool')) # checking how many were predicted correctly\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        # --- TRAINING ---\n",
    "\n",
    "        # collecting cost for each epoch for plotting\n",
    "        total_cost = []\n",
    "        init_op = tf.global_variables_initializer()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            sess.run(init_op)\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "\n",
    "                _, c = sess.run([optimizer, cost_function], feed_dict={X:x_train, y:y_train})\n",
    "                total_cost.append(c)\n",
    "\n",
    "                if (epoch+1) % 1000 == 0:\n",
    "                    print(\">> EPOCH:\", (epoch+1), \"Cost =\", \"{:.9f}\".format(c))\n",
    "\n",
    "            a = sess.run(accuracy, feed_dict={X: x_test, y: y_test})\n",
    "            total_accuracy.append(a)\n",
    "            print(\">>> Accuracy =\", \"{:.5f}%\".format(a*100))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set variables and placeholders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-236bed5d5b26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# declare training data placeholders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# input x1, x2, x3, ..., x12 (12 nodes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# output (5 nodes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n' is not defined"
     ]
    }
   ],
   "source": [
    "# n_hidden_layers = 1 # default is 1\n",
    "learning_rate = 0.01\n",
    "epochs = 10000 # cycles of feed forward + backprop\n",
    "epsilon = 0.000001 # used to avoid \"nan\" values from log in cost function\n",
    "\n",
    "# used to observe the change in accuracy as number of perceptrons increases\n",
    "init_perceptrons = 2\n",
    "total_perceptrons = 2\n",
    "step = 25\n",
    "\n",
    "# declare training data placeholders\n",
    "X = tf.placeholder(tf.float32, [None, n]) # input x1, x2, x3, ..., x12 (12 nodes)\n",
    "y = tf.placeholder(tf.float32, [None, k]) # output (5 nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run model\n",
    "# total_acc = run_model(X, y, n, epsilon, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation <a class=\"anchor\" id=\"evaluation\"></a>\n",
    "\n",
    "*Note: Include comparisson with other related work*\n",
    "\n",
    "| Paper | Method |\n",
    "|-------|--------|\n",
    "| [Automated Identification of High Impact Bug<br>Reports Leveraging Imbalanced Learning Strategies](http://ieeexplore.ieee.org.uproxy.library.dc-uoit.ca/stamp/stamp.jsp?arnumber=7552013&tag=1 \"Paper\") |  Naive Bayes Multinominal |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Conclusion <a class=\"anchor\" id=\"conclusion\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
