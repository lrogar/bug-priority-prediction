{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Data Extraction\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_data(data_path):\n",
    "    \n",
    "    df = pd.read_csv(clean_data_path, sep=',', encoding='ISO-8859-1', header=None)\n",
    "    clean_data = np.array(df)\n",
    "\n",
    "    # get rid of rows containing \"nan\" in clean data file\n",
    "    rows_to_delete = []\n",
    "    for i, row in enumerate(clean_data):\n",
    "        for j, val in enumerate(row):\n",
    "            if (str(row[j]).strip() == 'nan'):\n",
    "                print(\"> Deleting row: \" + str(row))\n",
    "                rows_to_delete.append(i)\n",
    "                break\n",
    "    clean_data = np.delete(clean_data, rows_to_delete, 0)\n",
    "\n",
    "    # don't include the last column; where the labels are\n",
    "    x = (clean_data[:,:-1])\n",
    "\n",
    "    # retrieve the last column: the target/labels\n",
    "    # reshape from (m,) to (m,1), then convert into one-hot vector (m,k)\n",
    "    y = pd.get_dummies(clean_data[:,-1]).values # also converting to one-hot vector using pandas\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_class_distribution(array, predict_labels):\n",
    "    \n",
    "    dist = []\n",
    "    for elem in array: dist.append(np.argmax(elem))\n",
    "        \n",
    "    unique, counts = np.unique(dist, return_counts=True)\n",
    "    #print(unique)\n",
    "    \n",
    "    counts = [\"{:.2f}%\".format(num/len(dist)*100) for num in counts]\n",
    "\n",
    "    return (dict(zip(predict_labels, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "def oversample_data(x, y_onehot, alg='naive'):\n",
    "        \n",
    "    # convert y from one-hot to 1D\n",
    "    y = []\n",
    "    for elem in y_onehot: y.append(np.argmax(elem))\n",
    "\n",
    "    if alg=='smote':\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        x_oversampled, y_oversampled = SMOTE().fit_sample(x, y)\n",
    "    \n",
    "    elif alg=='adasyn':\n",
    "        from imblearn.over_sampling import ADASYN\n",
    "        x_oversampled, y_oversampled = ADASYN().fit_sample(x, y)\n",
    "        \n",
    "    elif alg=='naive':\n",
    "        from imblearn.over_sampling import RandomOverSampler\n",
    "        ros = RandomOverSampler(random_state=0)\n",
    "        x_oversampled, y_oversampled = ros.fit_sample(x, y)\n",
    "        \n",
    "    else:\n",
    "        print(\"ERROR: This is not a valid algorithm.\")\n",
    "\n",
    "    # convert y back into a one-hot vector\n",
    "    y_oversampled = pd.get_dummies(y_oversampled).values\n",
    "    \n",
    "    return x_oversampled, y_oversampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Testing and Training Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def split_data(data, labels, train_perc):\n",
    "    \n",
    "    test_perc = round(1-train_perc, 2)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels, train_size=train_perc,\n",
    "                                                        test_size=test_perc, random_state=42, stratify=labels)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/khanparwaz/2017/blob/master/ROC%20Curve%20Multiclass\n",
    "import os\n",
    "def plot_roc(y_test, y_predicted, feature_labels, epochs, perceptrons, accuracy, predict_labels):\n",
    "    \n",
    "    # creating file name and caption text\n",
    "    fig_caption = (\"Total Accuracy: {:.5f}%\".format(accuracy*100) + \"\\nNumber of EPOCHS: \" + str(epochs) + \"\\nTotal Number of Perceptrons (upper-bound): \"\n",
    "                      + str(perceptrons) + \"\\nFeatures Used: \")\n",
    "    \n",
    "    file_name_features = \"\"\n",
    "    for i, label in enumerate(feature_names):\n",
    "        file_name_features = file_name_features + str(label) + \"-\"\n",
    "        if (i == len(feature_names)-1):\n",
    "            fig_caption = fig_caption + \"and \" + label\n",
    "        else:\n",
    "            fig_caption = fig_caption + label + \", \"\n",
    "        \n",
    "    file_name = \"./results/roc-\" + file_name_features + str(epochs) + \"-\" + str(total_perceptrons)\n",
    "    \n",
    "    # calculate ROC\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for i in range(k):\n",
    "        fpr[i], tpr[i], _ = roc_curve((y_test)[:, i], (y_predicted)[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # plot the calculated ROC and AUC values for each class\n",
    "    plt.figure(figsize=(10,6))\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'darkblue', 'pink'])\n",
    "    for i, color in zip(range(k), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=3,\n",
    "                 label='{0} (AUC = {1:0.2f})'\n",
    "                 ''.format(predict_labels[i], roc_auc[i]))\n",
    "\n",
    "    # plot random guess ROC\n",
    "    plt.plot([0, 1], [0, 1], 'k--',color='red', lw=2, label='Random Guess')\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    # plt.annotate('Random Guess',(.5,.48),color='red')\n",
    "    plt.xlabel('False Positive Rate', fontsize=15)\n",
    "    plt.ylabel('True Positive Rate', fontsize=15)\n",
    "    plt.title(\"Receiver Operating Characteristic (ROC) Curve\\nfor each class in the network\", fontsize=18)\n",
    "    plt.legend(loc=\"lower right\", fontsize=13)\n",
    "    plt.grid()\n",
    "    plt.text(0, -0.4, fig_caption, fontsize=13)\n",
    "    \n",
    "    # if the file exists, don't overrite it. instead, add a number to the right.\n",
    "    i = 1\n",
    "    while os.path.exists('{}-({:d}).png'.format(file_name, i)):\n",
    "        i += 1\n",
    "    plt.savefig('{}-({:d}).png'.format(file_name, i))\n",
    "        \n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_activation_function(X, W, b, func='softmax'):\n",
    "    \n",
    "    if (func == 'softmax'): # softmax\n",
    "        return tf.nn.softmax(tf.add(tf.matmul(X, W), b))\n",
    "    if (func == 'relu'): # relu\n",
    "        return tf.nn.relu(tf.add(tf.matmul(X, W), b))\n",
    "    else: # sigmoid\n",
    "        return tf.sigmoid(tf.add(tf.matmul(X, W), b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cost(y, y_):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using multiple layers\n",
    "def get_output_layer(n_hidden_layers, X, n, k, n_perceptrons):\n",
    "    \n",
    "    layer_weights = []\n",
    "    \n",
    "    # input layer to first hidden layer\n",
    "    layer_weights.append({'W': tf.Variable(tf.random_normal([n, n_perceptrons])),\n",
    "                          'b': tf.Variable(tf.random_normal([n_perceptrons]))})\n",
    "    \n",
    "    # generate this many hidden layers\n",
    "    for i in range(n_hidden_layers):\n",
    "        layer_weights.append({'W': tf.Variable(tf.random_normal([n_perceptrons, n_perceptrons])),\n",
    "                              'b': tf.Variable(tf.random_normal([n_perceptrons]))})\n",
    "\n",
    "    # last hidden layer to output layer\n",
    "    layer_weights.append({'W': tf.Variable(tf.random_normal([n_perceptrons, k])),\n",
    "                          'b': tf.Variable(tf.random_normal([k]))})\n",
    "            \n",
    "    # calculate output-first hidden inner layer\n",
    "    aggregated_val = apply_activation_function(X, layer_weights[0]['W'], layer_weights[0]['b'])\n",
    "    \n",
    "    # print(\"  aggregated_val.shape: \" + str(aggregated_val.shape))\n",
    "    \n",
    "    # calculate all hidden layers and output layer\n",
    "    for i in range(1, len(layer_weights)):\n",
    "        aggregated_val = apply_activation_function(aggregated_val, layer_weights[i]['W'], layer_weights[i]['b'])\n",
    "    \n",
    "    # return final layer\n",
    "    return aggregated_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from scipy import interp\n",
    "def run_model(n_hidden_layers, X, y, n, learning_rate, epochs, k,\n",
    "              init_perceptrons, total_perceptrons, step, feature_labels, predict_labels):\n",
    "   \n",
    "    # to store the different accuracy values for each number of perceptrons used\n",
    "    total_accuracy = []\n",
    "    \n",
    "    # collect precision, recall, accuracy and f1 measures\n",
    "    # precision_recall_fscore = []\n",
    "    \n",
    "    # if we are only trying with one set of perceptrons, adjust the upper bound for the \"range\" function below\n",
    "    if (init_perceptrons == total_perceptrons):\n",
    "        stop_cond = init_perceptrons + 1\n",
    "    # otherwise, set the upper bound taking into accout both the initial perceptrons, and the total wanted\n",
    "    else:\n",
    "        stop_cond = init_perceptrons + total_perceptrons + 1\n",
    "\n",
    "    # perform the training for each number of perceptrons specified\n",
    "    for n_nodes in range(init_perceptrons, stop_cond, step):\n",
    "\n",
    "        print(\"> Using \", n_nodes, \" perceptrons and \" + str(n_hidden_layers) + \" hidden layer(s) ...\")\n",
    "\n",
    "        y_ = get_output_layer(n_hidden_layers, X, n, k, n_nodes)\n",
    "        cost_function = get_cost(y, y_)\n",
    "        \n",
    "        # using gradient descent to minimize the cost\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1)) # checking how many were predicted correctly\n",
    "        benchmark_prediction = tf.equal(tf.argmax(y_rand, 1), tf.argmax(y, 1))\n",
    "        \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        benchmark_accuracy = tf.reduce_mean(tf.cast(benchmark_prediction, tf.float32))\n",
    "\n",
    "        # --- TRAINING ---\n",
    "\n",
    "        # collecting cost for each epoch for plotting\n",
    "        total_cost = []\n",
    "        init_op = tf.global_variables_initializer()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            sess.run(init_op)\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "\n",
    "                _, c = sess.run([optimizer, cost_function], feed_dict={X:x_train, y:y_train})\n",
    "                total_cost.append(c)\n",
    "\n",
    "                if (epoch+1) % 1000 == 0:\n",
    "                    print(\"  EPOCH:\", (epoch+1), \"Cost =\", \"{:.15f}\".format(c))\n",
    "\n",
    "            a = sess.run(accuracy, feed_dict={X: x_test, y: y_test})\n",
    "            b_a = sess.run(benchmark_accuracy, feed_dict={y: y_test})\n",
    "            total_accuracy.append(a)\n",
    "              \n",
    "            # --------------- CALCULATE PRECISION, RECALL, AND F-SCORE ---------------\n",
    "            # y_predicted = (y_.eval(feed_dict={X: x_test}))\n",
    "            # y_predicted = np.argmax(y_predicted, axis=1)\n",
    "            \n",
    "            # b = np.zeros((y_test.shape[0], k))\n",
    "            # b[np.arange(y_test.shape[0]), y_predicted] = 1\n",
    "            \n",
    "            # precision_recall_fscore.append(precision_recall_fscore_support(y_test, b, average='macro'))\n",
    "            # ------------------------------------------------------------------------\n",
    "            \n",
    "            # ------------------------------ CALCULATE AND PLOT ROC ------------------------------\n",
    "            \n",
    "            # converting tensors into arrays\n",
    "            y_predicted = (y_.eval(feed_dict={X: x_test}))\n",
    "            y_predicted = np.argmax(y_predicted, axis=1)\n",
    "            \n",
    "            y_predicted_onehot = np.zeros((y_test.shape[0], k)).astype(int)\n",
    "            y_predicted_onehot[np.arange(y_test.shape[0]), y_predicted] = 1\n",
    "            \n",
    "            # generate roc curve plot\n",
    "            plot_roc(y_test, y_predicted_onehot, feature_names, epochs, total_perceptrons, a, predict_labels)\n",
    "            \n",
    "            # ------------------------------------------------------------------------------------\n",
    "            \n",
    "            print(\"\\n  >> Accuracy = \" + \"{:.5f}%\".format(a*100) + \" vs. Random = \" + \"{:.5f}%\".format(b_a*100))\n",
    "            \n",
    "    return total_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project_name = 'all data'\n",
    "clean_data_path = \"../dataset/clean_data.csv\"\n",
    "# clean_data_path = \"../dataset/\" + project_name + \"_clean_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y = fetch_data(clean_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only include relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Features ['type' 'summary' 'description']\n"
     ]
    }
   ],
   "source": [
    "feature_names = [\"type\", \"reporter\", \"summary\", \"description\", \"description_words\"]\n",
    "predict_labels = ['Blocker', 'Critical', 'Major', 'Minor', 'Trivial']\n",
    "\n",
    "to_keep = [0, 2, 3] # keeping: type and description\n",
    "\n",
    "to_delete = np.delete(np.arange(5), to_keep)\n",
    "\n",
    "x = np.delete(x, to_delete, axis=1)\n",
    "feature_names = np.delete(feature_names, to_delete)\n",
    "\n",
    "print(\"Using Features \" + str(feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtain the class distribution of the data, and adjust it if it's imbalanced.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Project: ALL DATA\n",
      "\n",
      "Data Distribution\n",
      "{'Blocker': '1.20%', 'Critical': '4.12%', 'Major': '65.00%', 'Minor': '25.07%', 'Trivial': '4.60%'}\n"
     ]
    }
   ],
   "source": [
    "dist = get_class_distribution(y, predict_labels)\n",
    "#{'Trivial': 4.6, 'Major': 65.0, 'Critical': 4.125, 'Blocker': 1.2, 'Minor': 25.074999999999996} \n",
    "\n",
    "print(\"\\nProject: \" + (project_name).upper())\n",
    "print(\"\\nData Distribution\")\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alg = 'smote' # naive, smote, adasyn\n",
    "x, y = oversample_data(x, y, alg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Project: ALL DATA\n",
      "\n",
      "Data Distribution\n",
      "{'Blocker': '20.00%', 'Critical': '20.00%', 'Major': '20.00%', 'Minor': '20.00%', 'Trivial': '20.00%'}\n"
     ]
    }
   ],
   "source": [
    "dist = get_class_distribution(y, predict_labels)\n",
    "\n",
    "print(\"\\nProject: \" + (project_name).upper())\n",
    "print(\"\\nData Distribution\")\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Declare variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden_layers = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 10000 # cycles of feed forward + backpropagation\n",
    "\n",
    "# used to observe the change in accuracy as number of perceptrons increases\n",
    "init_perceptrons = 2\n",
    "total_perceptrons = 350\n",
    "step = 25 # changing from init_perceptrons to total_perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split data into training and testing sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> m (training samples) = 9100\n",
      "> n (num. features)= 3\n",
      "> k (num. classes) = 5\n"
     ]
    }
   ],
   "source": [
    "train_perc = .7 # percentage of total data used for training\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, train_perc) # randomly splitting up the data\n",
    "\n",
    "# setting m, n and k variables for placeholder definitions later on\n",
    "m = x_train.shape[0] # number of tuples for training\n",
    "n = x.shape[1] # number of features\n",
    "k = len(y[0]) # number of classes\n",
    "\n",
    "print(\"> m (training samples) = \" + str(m) + \"\\n> n (num. features)= \" + str(n) + \"\\n> k (num. classes) = \" + str(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based on the testing set, generate a random solution as a benchmark for comparisson in terms of accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> y_rand shape: (3900, 5)\n"
     ]
    }
   ],
   "source": [
    "y_rand = pd.get_dummies((np.floor(np.random.rand(len(y_test), 1)*5).astype(int)).flatten()).values\n",
    "print(\"> y_rand shape: \" + str(y_rand.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the neural network model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declare training data placeholders\n",
    "X = tf.placeholder(tf.float32, [None, n]) # input x1, x2, x3, ..., x12 (12 nodes)\n",
    "y = tf.placeholder(tf.float32, [None, k]) # output (5 nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Using  2  perceptrons and 1 hidden layer(s) ...\n",
      "  EPOCH: 1000 Cost = 1.629006743431091\n",
      "  EPOCH: 2000 Cost = 1.619009375572205\n",
      "  EPOCH: 3000 Cost = 1.615231394767761\n",
      "  EPOCH: 4000 Cost = 1.613701224327087\n",
      "  EPOCH: 5000 Cost = 1.612988948822021\n",
      "  EPOCH: 6000 Cost = 1.612613797187805\n",
      "  EPOCH: 7000 Cost = 1.612390875816345\n",
      "  EPOCH: 8000 Cost = 1.612227678298950\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-ddab2e5f0065>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# run model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m total_acc = run_model(n_hidden_layers, X, y, n, learning_rate, epochs, k, init_perceptrons,\n\u001b[0;32m----> 3\u001b[0;31m                         total_perceptrons, step, feature_names, predict_labels)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(precision_recall_fscore)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-00b0d15a38e4>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(n_hidden_layers, X, y, n, learning_rate, epochs, k, init_perceptrons, total_perceptrons, step, feature_labels, predict_labels)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0mtotal_cost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run model\n",
    "total_acc = run_model(n_hidden_layers, X, y, n, learning_rate, epochs, k, init_perceptrons,\n",
    "                        total_perceptrons, step, feature_names, predict_labels)\n",
    "\n",
    "# print(precision_recall_fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting accuracy (only if the number of perceptrons varies)\n",
    "if (init_perceptrons < total_perceptrons):\n",
    "    \n",
    "    perceptron_count = range(init_perceptrons, init_perceptrons + total_perceptrons + 1, step)\n",
    "    \n",
    "    avg_acc = np.mean(total_acc)\n",
    "\n",
    "    max_acc_index = np.argmax(total_acc)\n",
    "    max_acc = total_acc[max_acc_index]\n",
    "    \n",
    "    fig_caption = (\"Average Accuracy: {:.5f}%\".format(avg_acc*100) + \"\\nMaximum Accuracy: {:.5f}%\".format(max_acc*100)\n",
    "                   + \" with \" + str(perceptron_count[max_acc_index]) + \" perceptrons\")\n",
    "    title= 'Change of prediction accuracy\\nas the number of perceptrons increases'\n",
    "    file_name = './results/accuracy-perceptrons-' + str(init_perceptrons) + '-to-' + str(total_perceptrons)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(perceptron_count, total_acc, lw=3, color='red')\n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.xlabel(\"Number of perceptrons in hidden layer\", fontsize=15)\n",
    "    plt.ylabel(\"Accuracy (%)\", fontsize=15)\n",
    "    plt.grid()\n",
    "    plt.ylim(ymin=0)\n",
    "    plt.text(0, -0.055, fig_caption, fontsize=13)\n",
    "    \n",
    "    # if the file exists, don't overrite it. instead, add a number to the right.\n",
    "    i = 1\n",
    "    while os.path.exists('{}-({:d}).png'.format(file_name, i)):\n",
    "        i += 1\n",
    "    plt.savefig('{}-({:d}).png'.format(file_name, i))\n",
    "    \n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
